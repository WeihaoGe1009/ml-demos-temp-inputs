{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "You can just click **Runtime → Run all** to try the demo."
      ],
      "metadata": {
        "id": "OTHpdriMWKJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 3 - Markov Chain: Language, Probability & Illusion"
      ],
      "metadata": {
        "id": "lM4aICJ-WKOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this module, we explore how Markov chains, despite their simplicity, can generate outputs that resemble language, conversation, or even music. By modeling transitions between elements—letters, words, or notes—purely based on observed probabilities, these systems can create sequences that feel meaningful, even though no understanding is involved. This invites us to reflect on how easily structure and pattern can give the illusion of intent or intelligence."
      ],
      "metadata": {
        "id": "8RM70YzwWKmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title system setup and data loading {display-mode: \"form\"}\n",
        "# install packages\n",
        "!pip install --quiet nltk wordcloud matplotlib ipywidgets\n",
        "\n",
        "# import packages\n",
        "import nltk\n",
        "nltk.download('brown') # download the standard Brown Corpus training data set for Markov Chain\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from collections import defaultdict, Counter\n",
        "import random\n",
        "import string\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.colors as mcolors\n",
        "import textwrap\n",
        "\n",
        "from ipywidgets import Output\n",
        "import asyncio\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "5nO8p4lnWZ5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## the Brown Corpus Dataset\n",
        "\n",
        "The Brown Corpus is a widely used text dataset compiled from American English sources in the 1960s. It includes documents from various genres—such as news, fiction, government, and academic writing—making it useful for studying general language patterns. In this demo, we use it as training data for a simple character-level Markov model. Like the MNIST data set we've seen before, it was implemented in another publicly-available Python package called nltk (natural language ToolKit)"
      ],
      "metadata": {
        "id": "X9Oh5G5i9lYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1: Markov Chain\n",
        "A Markov chain is a mathematical model where the next state depends only on the current state, not the full history. First introduced by Russian mathematician Andrey Markov in the early 1900s, it was originally used to study patterns in poetry. Today, Markov chains are widely used in fields like natural language processing, genetics, and economics. In text generation, a Markov model learns the probability of each character following a given context and uses that to generate new text, one character at a time."
      ],
      "metadata": {
        "id": "lMhNjbBK9sSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train a simple Markov Chain model, which probablisticly selects the next letter {display-mode: \"form\"}\n",
        "\n",
        "# Prep character sequence\n",
        "text = ' '.join(brown.words()).lower()\n",
        "text = ''.join([c for c in text if c in string.ascii_lowercase + \" .,;:!?'\\n\"])\n",
        "text = text.replace('\\n', ' ')  # unify whitespace\n",
        "\n",
        "# Build 3-gram model: P(c_i | c_{i-3}, c_{i-2}, c_{i-1})\n",
        "def train_char_markov(text, order=3):\n",
        "    model = defaultdict(Counter)\n",
        "    for i in range(order, len(text)):\n",
        "        context = text[i - order:i]\n",
        "        next_char = text[i]\n",
        "        model[context][next_char] += 1\n",
        "    # Normalize to probabilities\n",
        "    for ctx, counter in model.items():\n",
        "        total = sum(counter.values())\n",
        "        model[ctx] = {c: count / total for c, count in counter.items()}\n",
        "    return model\n",
        "\n",
        "char_model = train_char_markov(text, order=3)"
      ],
      "metadata": {
        "id": "5d32oDD5WuSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Now we can run the trained Markov model to generate a random letter string {display-mode: \"form\"}\n",
        "\n",
        "def generate_trace(model, order=3, steps=30):\n",
        "    seed = random.choice(list(model.keys()))\n",
        "    output = seed\n",
        "    trace = []\n",
        "\n",
        "    for _ in range(steps):\n",
        "        context = output[-order:]\n",
        "        probs = model.get(context, {' ': 1.0})  # fallback to space\n",
        "        next_char = random.choices(list(probs.keys()), weights=probs.values())[0]\n",
        "        trace.append((context, probs, next_char))\n",
        "        output += next_char\n",
        "    return output, trace"
      ],
      "metadata": {
        "id": "iVNGfPe7W5bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run the Markov Code {display-mode: \"both\"}\n",
        "steps=30 # change this value if you want to generate letter series of different lengths\n",
        "output_string, trace = generate_trace(char_model, steps=steps)"
      ],
      "metadata": {
        "id": "WVQt9uu_9TBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title visualize the running results {display-mode: \"form\"}\n",
        "\n",
        "\n",
        "# Define consistent character axis\n",
        "full_charset = list(string.ascii_lowercase) + [' ', '.', ',', ';', ':', '!', '?', \"'\"]\n",
        "CHAR_X = full_charset + ['[END]']\n",
        "\n",
        "out = Output()\n",
        "display(out)\n",
        "\n",
        "\n",
        "cmap = cm.get_cmap('Blues')  # Darker = higher\n",
        "\n",
        "def prob_to_color(p):\n",
        "    # Manually enforce 0.0–1.0 scaling\n",
        "    p_clipped = max(0.0, min(1.0, float(p)))\n",
        "    rgba = cmap(p_clipped)\n",
        "    return mcolors.to_hex(rgba)\n",
        "\n",
        "def render_char_bar(i):\n",
        "    context, probs, selected = trace[i]\n",
        "    sentence_so_far = ''.join([t[2] for t in trace[:i+1]])\n",
        "\n",
        "    # Prepare fixed bars\n",
        "    prob_list = [probs.get(c, 0.0) for c in CHAR_X]\n",
        "    x = list(range(len(CHAR_X)))\n",
        "\n",
        "    colors = []\n",
        "\n",
        "    for c, p in zip(CHAR_X, prob_list):\n",
        "        colors.append(prob_to_color(p)) # blue shade for probability\n",
        "\n",
        "    # === Diagnostic check for alignment ===\n",
        "    assert len(prob_list) == len(CHAR_X) == len(colors), \"Length mismatch\"\n",
        "    for idx, (c, p, col) in enumerate(zip(CHAR_X, prob_list, colors)):\n",
        "        expected = prob_to_color(p)\n",
        "        assert col == expected, f\"Mismatch at {c}: prob={p:.3f}, color={col} ≠ {expected}\"\n",
        "    # === End diagnostic block ===\n",
        "\n",
        "    with out:\n",
        "        out.clear_output(wait=True)\n",
        "        fig, ax = plt.subplots(figsize=(12, 4))\n",
        "        ax.bar(x, prob_list, color=colors)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_xlim(-0.5, len(CHAR_X) - 0.5)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_ylabel('P(next char)', fontsize=12)\n",
        "        ax.set_title(f\"Step {i+1} — Generated so far: {sentence_so_far!r}\",\n",
        "                     loc='left', fontsize=13)\n",
        "        fig.subplots_adjust(left=0.05, right=0.95, top=0.85, bottom=0.2)\n",
        "        plt.show()\n",
        "\n",
        "async def autoplay_trace(pause=0.2):\n",
        "    for i in range(len(trace)):\n",
        "        render_char_bar(i)\n",
        "        await asyncio.sleep(pause)\n",
        "\n",
        "await autoplay_trace()"
      ],
      "metadata": {
        "id": "i_Ket6V4XC-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How a Markov Chain Generates Text\n",
        "\n",
        "This animation shows how a character-level Markov chain generates text one letter at a time. The bar height and color represent the probability of each next character (letters, spaces, punctuation, etc.).\n",
        "\n",
        "At each step, the model updates the probabilities based on the current context, then randomly selects the next character. Some word-like patterns emerge, but the full sentence often lacks meaning.\n",
        "\n",
        "This highlights that the machine is not choosing words based on meaning, but simply following learned probabilities. Larger models can produce more human-like text, but the process remains fundamentally probabilistic."
      ],
      "metadata": {
        "id": "VeARMv1G-Og7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2: Search vs Generation - \"A Confused Parrot\"\n",
        "\n",
        "Nowadays, since large GenAI models such as ChatGPT, Gemni, Copilot, etc. are providing plausible answers, many people starts to use them as a search engine. Moreover, now search engine websites like google or bing are integrating AI with their search, providing summaries on the results. The difference between GenAI and Search Engine starts to blur to many users. However, from the demo above, we have prove that the generative models do not generate text based on their meanings, but based on the probability calculated from previous letter/words.\n",
        "\n",
        "In this section, we are going to explore the difference between a search engine and a generative model, which we call **parrot\\_search** and **parrot\\_markov**.\n",
        "\n",
        "Both parrots will learn from a tiny knowledge base composed of the wikipedia pages of Bach, Haydn, Mozart, and Beethoven. Then we'll see how the parrots will respond to our inputs and observe the difference between retrieving information and generating it."
      ],
      "metadata": {
        "id": "oo265MPHAy4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### obtaining prepared files\n",
        "Directly generate searchbase and training Markov chain will take a long time. Therefore, we have prepared some pre-generated files from the raw knowledge base to facilitate the Parrots to work faster. Moreover, some algorithms are complex and therefore we don't include them in this notebook, but rather prepared some pre-written codes to import.\n",
        "Let's download these files."
      ],
      "metadata": {
        "id": "5kArWvq7cOYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download files {display-mode: \"form\"}\n",
        "url_header = \"https://raw.githubusercontent.com/WeihaoGe1009/ml-demos-temp-inputs/main/03_markov/\"\n",
        "\n",
        "\n",
        "files_to_download = {\n",
        "    \"parrots.py\": url_header + \"scripts/parrots.py\",\n",
        "    \"bwt_input_bytes.pkl\": url_header + \"data/bwt_index/bwt_input_bytes.pkl\",\n",
        "    \"sa.pkl\": url_header + \"data/bwt_index/sa.pkl\",\n",
        "    \"paragraph_offsets.pkl\": url_header + \"data/bwt_index/paragraph_offsets.pkl\",\n",
        "    \"paragraph_map.pkl\": url_header + \"data/bwt_index/paragraph_map.pkl\",\n",
        "    \"markov_model.pkl\": url_header + \"data/markov_model.pkl\"\n",
        "}\n",
        "\n",
        "for filename, url in files_to_download.items():\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"Downloading {filename}...\")\n",
        "        os.system(f\"wget {url}\")\n",
        "    else:\n",
        "        print(f\"{filename} already exists. Skipping.\")"
      ],
      "metadata": {
        "id": "OzEtZs399Rub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing the parrots:\n",
        "* the **parrot\\_search** looks up the original knowledge base for matching word combinations. On the other hand, the **parrot\\_markov** calculated what the probability is for a word to follow another, based on the original knowledge base, and generate text probablistically.\n",
        "* only **parrot\\_search** will be able to say \"I don't know\" when it couldn't find anything in the database.\n",
        "* **parrot\\_markov**, on the other hand, always has something to say. it will generate words by probability without realizing any meaning, since the meaning is not something learnt in its implementation. This is so-called \"illusion\" in the modern GenAI, where generative AIs are \"confidently\" creating unreliable answers.\n"
      ],
      "metadata": {
        "id": "_b5UNFXwB4ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set up Parrot_Search {display-mode: \"form\"}\n",
        "# set up the pre-written Parrot_Search package and load the index files\n",
        "# the index files are transformed from original knowledge base text,\n",
        "# which are created to facilitate search\n",
        "from parrots import ParrotSearchBWT, multi_keyword_search, highlight_keywords\n",
        "\n",
        "parrot_search = ParrotSearchBWT(\n",
        "    \"bwt_input_bytes.pkl\",\n",
        "    \"sa.pkl\",\n",
        "    \"paragraph_offsets.pkl\",\n",
        "    \"paragraph_map.pkl\"\n",
        ")"
      ],
      "metadata": {
        "id": "SNTFo_DlxIDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set up Parrot_Marokv {display-mode: \"form\"}\n",
        "from parrots import ParrotMarkovPhrase, generate_phrase_text\n",
        "parrot_markov = ParrotMarkovPhrase(\"markov_model.pkl\")"
      ],
      "metadata": {
        "id": "wQsc1xTkjF9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Compare Parrot_Search and Parrot_Markov {display-mode: \"form\"}\n",
        "\n",
        "\n",
        "# you can change the values in keywords\n",
        "keywords = ['Bach', '1750', 'German']\n",
        "\n",
        "keywords = [kw.lower() for kw in keywords] # changing everything to lowercase\n",
        "\n",
        "search_results = multi_keyword_search(parrot_search, keywords)\n",
        "for r in search_results:\n",
        "    #print(\"Article:\", r[\"article\"])\n",
        "    wrapped_search_text = textwrap.fill(highlight_keywords(r[\"text\"], keywords), width=80)\n",
        "    print(wrapped_search_text)\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "generation_results = generate_phrase_text(parrot_markov, keywords)\n",
        "wrapped_generation_text = textwrap.fill(highlight_keywords(generation_results, keywords), width=80)\n",
        "print(wrapped_generation_text)"
      ],
      "metadata": {
        "id": "5v4_qiLGlCSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is the difference between our parrots?\n",
        "* **Mechanism**\n",
        "  * *parrot_search*: Retrieves existing sentence that contains the keywords.\n",
        "  * *parrot_markov*: Generates a new paragraph based on phrase-to-phrase patterns learned from the corpus.\n",
        "\n",
        "* **Source of Output**\n",
        "  * *parrot_search: Output is taken directly from the original knowledge base (Wikipedia pages).\n",
        "  * *parrot_markov*: Output is synthesized and does not exist as-is in the corpus.\n",
        "\n",
        "* **Keyword Coverage**\n",
        "  * *parrot_search*: Must contain the exact keywords.\n",
        "  * *parrot_markov*: May or may not include all the keywords.\n",
        "\n",
        "* **Output Uniqueness**\n",
        "  * *parrot_search*: The same sentence can be found verbatim in the corpus.\n",
        "  * *parrot_markov*: The generated text is unlikely to appear word-for-word in the original data.\n",
        "\n",
        "* **Factuality**\n",
        "  * *parrot_search*: The content is real, but may be contextually irrelevant.\n",
        "  * *parrot_markov*: The output may sound plausible, but is not guaranteed to be correct or real.\n"
      ],
      "metadata": {
        "id": "ugpPaNbbu2Fg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3: Markov Musician - optional\n",
        "\n",
        "Generative models not only works on texts. It can generate images, music, voice, videos, etc. Here we are going to explore a tiny simple case using existing package to generate a tiny excerpt of music."
      ],
      "metadata": {
        "id": "aghQzRMPCXeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title install music-relevant packages (a bit slow) {display-mode: \"form\"}\n",
        "# Install system-level FluidSynth (needed by pyfluidsynth and pretty_midi)\n",
        "!apt-get install -y -qq fluidsynth > /dev/null\n",
        "\n",
        "!pip install -q pretty_midi mido pyfluidsynth\n"
      ],
      "metadata": {
        "id": "OMCa3XsWue1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate a music sequence based on a \"prompt\" of C major scale {display-mode: \"form\"}\n",
        "import numpy as np\n",
        "import pretty_midi\n",
        "\n",
        "# Define a simple note vocabulary (C major scale)\n",
        "notes = ['C4', 'D4', 'E4', 'F4', 'G4', 'A4', 'B4']\n",
        "note_to_pitch = {note: pretty_midi.note_name_to_number(note) for note in notes}\n",
        "pitch_to_note = {v: k for k, v in note_to_pitch.items()}\n",
        "\n",
        "# Create a simple 1st-order Markov transition matrix with uniform probabilities\n",
        "transition_matrix = np.ones((len(notes), len(notes))) / len(notes)\n",
        "\n",
        "# Generate a note sequence\n",
        "np.random.seed(42)\n",
        "sequence = []\n",
        "current_note = np.random.choice(notes)\n",
        "sequence.append(current_note)\n",
        "\n",
        "for _ in range(32):  # 32 notes long\n",
        "    i = notes.index(current_note)\n",
        "    next_note = np.random.choice(notes, p=transition_matrix[i])\n",
        "    sequence.append(next_note)\n",
        "    current_note = next_note\n"
      ],
      "metadata": {
        "id": "KzdOVMlh1rXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert the text sequence to music format midi {display-mode: \"form\"}\n",
        "def create_midi(note_sequence, output_path=\"markov_music.mid\", duration=0.5):\n",
        "    pm = pretty_midi.PrettyMIDI()\n",
        "    instrument = pretty_midi.Instrument(program=0)\n",
        "\n",
        "    start_time = 0\n",
        "    for note in note_sequence:\n",
        "        pitch = note_to_pitch[note]\n",
        "        note_obj = pretty_midi.Note(\n",
        "            velocity=100, pitch=pitch,\n",
        "            start=start_time, end=start_time + duration\n",
        "        )\n",
        "        instrument.notes.append(note_obj)\n",
        "        start_time += duration\n",
        "\n",
        "    pm.instruments.append(instrument)\n",
        "    pm.write(output_path)\n",
        "    return output_path\n",
        "\n",
        "midi_path = create_midi(sequence)"
      ],
      "metadata": {
        "id": "YWC1bgoP1raE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Play the music {display-mode: \"form\"}\n",
        "import pretty_midi\n",
        "from IPython.display import Audio\n",
        "\n",
        "def midi_to_audio(midi_path):\n",
        "    pm = pretty_midi.PrettyMIDI(midi_path)\n",
        "    audio = pm.fluidsynth()\n",
        "    return Audio(audio, rate=22050)\n",
        "\n",
        "midi_to_audio(midi_path)\n"
      ],
      "metadata": {
        "id": "6D2EB7uB1ref"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hcFab6k34CyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take-home Messages\n",
        "* Markov-Chain model and other generative models produce a chain of output probablistically. These models do not learn the \"meanings\".\n",
        "* Generative model and search engine are different. Generative model do not look for matching patterns, but produces word after word probablistically\n",
        "* Same priciples applies to other forms of content. (but in biology, the word (6 kmer) frequency in the S16 ribosome is a feature good enough to tell species apart and led to the discovery of Archea. how about human's work, shall these frequencies be considered as a fingerprint for people's art work, so that if the machine learns, it is violating their rights? or it just fingerprints style, which is not protected by the IP law, but still violates other rights maybe? or what if the machine learns from many different people, can we say these rights are not violated since now the frequency does not match to anyone's fingerprinting prequencies? this is counter-intuitive to Author G who does not know IP law very well, like, stealing from one person is stealing while stealing from 400 people becomes original?)"
      ],
      "metadata": {
        "id": "28E-U-LLCtdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Comments\n",
        "In biology, the frequency of short DNA sequences—like 6-mers in the 16S ribosomal RNA—can distinguish species and even led to the discovery of Archaea.\n",
        "\n",
        "Could word or style frequencies in human creative work function in a similar way—like a fingerprint of artistic identity?\n",
        "\n",
        "Even if such fingerprinting works, it can be blurred by training on different input data. That leads to the question:\n",
        "* If a machine learns patterns from one artist, we might say it copies.\n",
        "* If it learns from 400 and the patterns become unclear, does it become “original”?\n"
      ],
      "metadata": {
        "id": "PDofk3YC75Xs"
      }
    }
  ]
}