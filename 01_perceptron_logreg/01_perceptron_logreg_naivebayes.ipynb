{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMnW+UFTf9vLduf1aX35NPB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Module 1 - Classification\n","This notebook explores how early machine learning models can classify text as poetic or legal based on word patterns.\n","\n","We use:\n","\n","* Poetic text from selected Shakespeare sonnets\n","\n","* Legal text from the U.S. Constitution (Articles I and II)\n","\n","Each sentence is labeled (0 = poetic, 1 = legal), and converted to numbers using a Bag-of-Words approach — counting how often each word appears.\n","\n","We demonstrate three models:\n","\n","* Perceptron (classic, mistake-driven) manually implemented and using existing package\n","\n","* Logistic Regression (probabilistic, weight-based)\n","\n","* Naive Bayes (probabilistic, frequency-based)\n","\n","These models show how early AI learned to classify text — without understanding meaning."],"metadata":{"id":"Yhog9KqJJ1Wi"}},{"cell_type":"markdown","source":["## Install and import Packages"],"metadata":{"id":"RicFmTt1KCrn"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CVpm9xmHEvW6","executionInfo":{"status":"ok","timestamp":1749657985292,"user_tz":300,"elapsed":3525,"user":{"displayName":"zweige1003","userId":"02327896534499890410"}},"outputId":"260f1d02-5052-4bfc-aa08-246ea81b1c51","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"]}],"source":["#@title Install required packages {display-mode: \"form\"}\n","!pip install numpy matplotlib pandas scikit-learn"]},{"cell_type":"code","source":["#@title import required packages {display-mode: \"form\"}\n","import numpy as np\n","import pandas as pd\n","\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.linear_model import Perceptron, LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.naive_bayes import MultinomialNB\n","\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"0TI8hhv8Jxl9","executionInfo":{"status":"ok","timestamp":1749666536135,"user_tz":300,"elapsed":38,"user":{"displayName":"zweige1003","userId":"02327896534499890410"}}},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":["## Load and process data\n","* we first read the data from 01_perceptron_logreg/data/labeled_sentences.csv\n","* In this file, we have shuffled sentences from 10 arbitraily picked sonnets by Shakespear, and Article I of the US Constitution. When the sentence is from the sonnets (hence \"poetic\"), it is labeled as 0. Otherwise it is labeled as 1.\n","* With this data set, we can train some simple models to identify whether a sentence is in a more poetic tone or a more legal tone."],"metadata":{"id":"mQ7qWUL8NiUa"}},{"cell_type":"code","source":["# load data from the GitHub repo of the Demo\n","url_header = \"https://raw.githubusercontent.com/WeihaoGe1009/ml-demos-temp-inputs/main/\"\n","module_name = \"01_perceptron_logreg/\"\n","data_dir = url_header+module_name+'data/'\n","input_file = data_dir + 'labeled_sentences.csv'\n","df_data = pd.read_csv(input_file)"],"metadata":{"id":"Tm1izggRMSoS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show 5 lines of the input data\n","df_data.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"3kcAhXZrx_tx","executionInfo":{"status":"ok","timestamp":1749658682407,"user_tz":300,"elapsed":186,"user":{"displayName":"zweige1003","userId":"02327896534499890410"}},"outputId":"46fccc37-308c-4e5a-be29-1f95bbf53491"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            Sentence  Label\n","0           Pity the world, or else this glutton be—      0\n","1    If snow be white, why then her breasts are dun;      0\n","2  The Times, Places and Manner of holding Electi...      1\n","3     For fear of which, hear this, thou age unbred:      0\n","4         So, till the judgment that yourself arise,      0"],"text/html":["\n","  <div id=\"df-eebe49f9-9468-49cf-8451-2529ac5e41ba\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Pity the world, or else this glutton be—</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>If snow be white, why then her breasts are dun;</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The Times, Places and Manner of holding Electi...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>For fear of which, hear this, thou age unbred:</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>So, till the judgment that yourself arise,</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eebe49f9-9468-49cf-8451-2529ac5e41ba')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-eebe49f9-9468-49cf-8451-2529ac5e41ba button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-eebe49f9-9468-49cf-8451-2529ac5e41ba');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-174dac91-8641-4f04-be38-0e17865b555e\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-174dac91-8641-4f04-be38-0e17865b555e')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-174dac91-8641-4f04-be38-0e17865b555e button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_data","summary":"{\n  \"name\": \"df_data\",\n  \"rows\": 195,\n  \"fields\": [\n    {\n      \"column\": \"Sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 195,\n        \"samples\": [\n          \"With what I most enjoy contented least;\",\n          \"You live in this, and dwell in lovers\\u2019 eyes.\",\n          \"To coin Money, regulate the Value thereof, and of foreign Coin, and fix the Standard of Weights and Measures;\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["### Train-test split\n","it is important to split the input data into a training set and a test set (in some references, you can see there is a validation set as well). We train our models based on the training set. But since the model's parameters are calculated from the data in the training set, it would achieve very high accuracy using the same data. Therefore, we set aside some data before training, so that we can evaluate the model performance more objectively."],"metadata":{"id":"wL79Iy3QHNsl"}},{"cell_type":"code","source":["# train-test split\n","train_df, test_df = train_test_split(df_data, test_size=0.2, stratify=df_data['Label'], random_state=42)"],"metadata":{"id":"vKESGnOvDNVm","executionInfo":{"status":"ok","timestamp":1749663195125,"user_tz":300,"elapsed":38,"user":{"displayName":"zweige1003","userId":"02327896534499890410"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Vectorization - converint text to numbers\n","Computers do not \"see\" letters, they only deal with numbers. Therefore, we have to convert text to numbers. Here, we would use a very simple way: replacing the word by their counts in the sample. The frequencies of the words already carry some information."],"metadata":{"id":"7AAuYtfAHptr"}},{"cell_type":"code","source":["# First, we split sentences into words\n","def tokenize(text):\n","    return text.lower().split()  # basic word-by-word splitting\n","\n","# Then, we build the vocabulary from the training data only\n","# This is our list of known words — like the dictionary the model can learn from\n","vocab = sorted(set(word for sentence in train_df['Sentence'] for word in tokenize(sentence)))\n","word_to_index = {word: i for i, word in enumerate(vocab)}\n","\n","# then we use an algorithm called \"Bag of Words\",\n","# which encodes each word by how many time it appears\n","# Each sentence becomes a list of numbers — how often each word appears\n","def sentence_to_vector(sentence):\n","    vector = np.zeros(len(vocab))\n","    for word in tokenize(sentence):\n","        if word in word_to_index:\n","            vector[word_to_index[word]] += 1\n","    return vector\n","\n","# Now we convert all training and test sentences into vectors\n","X_train = np.array([sentence_to_vector(s) for s in train_df['Sentence']])\n","y_train = train_df['Label'].values  # 0 = poetic, 1 = legal\n","\n","X_test = np.array([sentence_to_vector(s) for s in test_df['Sentence']])\n","y_test = test_df['Label'].values"],"metadata":{"id":"6zq28d7qMk2c","executionInfo":{"status":"ok","timestamp":1749664927470,"user_tz":300,"elapsed":19,"user":{"displayName":"zweige1003","userId":"02327896534499890410"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## manual implementation of Perceptron\n","Perceptron is a very early AI model. Here we are going to train a Perceptron to distinguish between poetic and legal sentences. First, we turned each sentence into a list of word counts (Bag-of-Words). Then the model practiced by reading examples and adjusting whenever it guessed wrong. Finally, we tested the model on new sentences to see how well it learned."],"metadata":{"id":"UZI728msQJpe"}},{"cell_type":"code","source":["# manual implementation of perceptron\n","# Convert labels to Perceptron-friendly format: -1 and +1\n","\n","y_train_bin = np.where(y_train == 0, -1, 1)\n","\n","# Initialize a blank model (Perceptron)\n","weights = np.zeros(len(vocab))  # One value (or \"dial\") per word\n","bias = 0                        # Extra offset\n","learning_rate = 1.0\n","epochs = 10\n","min_mistakes = 2  # Stop training early if fewer than 2 mistakes."],"metadata":{"id":"x6KUT08VIwVE","executionInfo":{"status":"ok","timestamp":1749665063014,"user_tz":300,"elapsed":8,"user":{"displayName":"zweige1003","userId":"02327896534499890410"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Training — the model practices reading\n","for round in range(epochs):\n","    print(f\"Round {round + 1}\")\n","    mistakes = 0\n","    for i in range(len(X_train)):\n","        result = np.dot(weights, X_train[i]) + bias\n","\n","        # If the model makes a mistake, adjust its understanding\n","        if y_train_bin[i] * result <= 0:\n","            weights += learning_rate * y_train_bin[i] * X_train[i]\n","            bias += learning_rate * y_train_bin[i]\n","            mistakes += 1\n","            print(f\"  Incorrect on sentence {i}, updating model.\")\n","\n","    print(f\"  Total mistakes this round: {mistakes}\\n\")\n","\n","    # Early stopping condition\n","    if mistakes < min_mistakes:\n","        print(f\"Stopping early — only {mistakes} mistakes this round.\")\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8Es8x0iJJ1m","executionInfo":{"status":"ok","timestamp":1749665063541,"user_tz":300,"elapsed":4,"user":{"displayName":"zweige1003","userId":"02327896534499890410"}},"outputId":"176ab334-7419-438a-9cdc-173d6890dba4"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Round 1\n","  Incorrect on sentence 0, updating model.\n","  Incorrect on sentence 1, updating model.\n","  Incorrect on sentence 3, updating model.\n","  Incorrect on sentence 6, updating model.\n","  Incorrect on sentence 8, updating model.\n","  Incorrect on sentence 11, updating model.\n","  Incorrect on sentence 17, updating model.\n","  Incorrect on sentence 21, updating model.\n","  Incorrect on sentence 22, updating model.\n","  Incorrect on sentence 29, updating model.\n","  Incorrect on sentence 42, updating model.\n","  Incorrect on sentence 56, updating model.\n","  Incorrect on sentence 72, updating model.\n","  Incorrect on sentence 104, updating model.\n","  Incorrect on sentence 108, updating model.\n","  Incorrect on sentence 120, updating model.\n","  Incorrect on sentence 126, updating model.\n","  Incorrect on sentence 136, updating model.\n","  Incorrect on sentence 138, updating model.\n","  Total mistakes this round: 19\n","\n","Round 2\n","  Incorrect on sentence 17, updating model.\n","  Incorrect on sentence 99, updating model.\n","  Total mistakes this round: 2\n","\n","Round 3\n","  Incorrect on sentence 26, updating model.\n","  Incorrect on sentence 126, updating model.\n","  Total mistakes this round: 2\n","\n","Round 4\n","  Incorrect on sentence 30, updating model.\n","  Incorrect on sentence 138, updating model.\n","  Total mistakes this round: 2\n","\n","Round 5\n","  Incorrect on sentence 86, updating model.\n","  Incorrect on sentence 145, updating model.\n","  Total mistakes this round: 2\n","\n","Round 6\n","  Incorrect on sentence 125, updating model.\n","  Total mistakes this round: 1\n","\n","Stopping early — only 1 mistakes this round.\n"]}]},{"cell_type":"code","source":["# Use the trained model to predict legal vs poetic on the test set\n","def predict(X_input):\n","    result = np.dot(X_input, weights) + bias\n","    return np.where(result > 0, 1, 0)  # 1 = legal, 0 = poetic\n","\n","# Evaluate how well the model did\n","y_pred = predict(X_test)\n","accuracy = np.mean(y_pred == y_test)\n","print(f\"Test accuracy: {accuracy:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QK87gQDBJO0b","executionInfo":{"status":"ok","timestamp":1749665070441,"user_tz":300,"elapsed":11,"user":{"displayName":"zweige1003","userId":"02327896534499890410"}},"outputId":"fbefb430-f840-4148-d810-d6f6cdc0a78c"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Test accuracy: 0.97\n"]}]},{"cell_type":"code","source":["# Show a few prediction examples to the user\n","print(\"\\nSample Predictions:\")\n","num_examples = 5  # Show 5 test cases\n","for i in range(num_examples):\n","    sentence = test_df['Sentence'].iloc[i]\n","    true_label = \"Legal\" if y_test[i] == 1 else \"Poetic\"\n","    predicted_label = \"Legal\" if y_pred[i] == 1 else \"Poetic\"\n","\n","    print(f\"\\nSentence {i+1}:\")\n","    print(f\"  Text: \\\"{sentence}\\\"\")\n","    print(f\"  True label: {true_label}\")\n","    print(f\"  Predicted:  {predicted_label}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KTs9OLuKJY4N","executionInfo":{"status":"ok","timestamp":1749665072815,"user_tz":300,"elapsed":8,"user":{"displayName":"zweige1003","userId":"02327896534499890410"}},"outputId":"0845c53e-f0d2-4c65-aa92-64f7f55ba639"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Sample Predictions:\n","\n","Sentence 1:\n","  Text: \"No Title of Nobility shall be granted by the United States: And no Person holding any Office of Profit or Trust under them, shall, without the Consent of the Congress, accept of any present, Emolument, Office, or Title, of any kind whatever, from any King, Prince, or foreign State.\"\n","  True label: Legal\n","  Predicted:  Legal\n","\n","Sentence 2:\n","  Text: \"Past reason hated as a swallowed bait\"\n","  True label: Poetic\n","  Predicted:  Poetic\n","\n","Sentence 3:\n","  Text: \"When in eternal lines to time thou grow’st.\"\n","  True label: Poetic\n","  Predicted:  Poetic\n","\n","Sentence 4:\n","  Text: \"Of this our time, all you prefiguring;\"\n","  True label: Poetic\n","  Predicted:  Poetic\n","\n","Sentence 5:\n","  Text: \"To make Rules for the Government and Regulation of the land and naval Forces;\"\n","  True label: Legal\n","  Predicted:  Legal\n"]}]},{"cell_type":"code","source":["# Now: Try your own sentence!\n","# Change the sentence below to test what the model predicts\n","input_sentence = \"This Constitution shall be the supreme Law of the Land.\"\n","\n","# Convert it to a bag-of-words vector using the training vocabulary\n","input_vector = sentence_to_vector(input_sentence)\n","\n","# Predict using the trained model\n","result = np.dot(weights, input_vector) + bias\n","predicted_label = \"Legal\" if result > 0 else \"Poetic\"\n","\n","print(\"\\nYour sentence:\")\n","print(f\"  \\\"{input_sentence}\\\"\")\n","print(f\"Predicted label: {predicted_label}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"byV5RL6qJfQi","executionInfo":{"status":"ok","timestamp":1749665286402,"user_tz":300,"elapsed":5,"user":{"displayName":"zweige1003","userId":"02327896534499890410"}},"outputId":"caae7b1e-0d48-4edb-c2fa-fab3962a3063"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Your sentence:\n","  \"This Constitution shall be the supreme Law of the Land.\"\n","Predicted label: Legal\n"]}]},{"cell_type":"markdown","source":["After training, we can ask the model to label new sentences it has never seen before, we compare it to the correct answer. This helps us see where the model is working well and where it might still be confused."],"metadata":{"id":"plJ1sWCtTHxi"}},{"cell_type":"markdown","source":["## Perceptron from package\n","* in real practice, it is not necessary to implement everything from beginning. We load packages which does the mathematics for us and optimized for the performance\n","* This version uses a built-in Perceptron from a well-known machine learning toolkit called scikit-learn. It handles all the weight updates and training process internally. We give it the same word-count data (Bag-of-Words) and let it learn how to tell poetic and legal sentences apart."],"metadata":{"id":"7uMpweBGQWyc"}},{"cell_type":"code","source":["# Train the model using scikit-learn's Perceptron\n","clf = Perceptron(max_iter=10, tol=1e-3, random_state=42)\n","clf.fit(X_train, y_train)  # y_train stays as 0/1 — no need to convert to -1/1\n","\n","# Predict on the test set\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Scikit-learn Perceptron Test Accuracy: {accuracy:.2f}\")"],"metadata":{"id":"03WBYizwQXEN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749665216673,"user_tz":300,"elapsed":41,"user":{"displayName":"zweige1003","userId":"02327896534499890410"}},"outputId":"8749df6d-3d37-432d-bf74-6c94fb02e7f0"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Scikit-learn Perceptron Test Accuracy: 0.97\n"]}]},{"cell_type":"code","source":["# Now: Try your own sentence!\n","# Change the sentence below to test what the model predicts\n","input_sentence = \"Shall I compare thee to a summer's day?\"\n","\n","# Convert it to a bag-of-words vector using the training vocabulary\n","input_vector = sentence_to_vector(input_sentence)\n","\n","# Predict using the trained model\n","predicted_label = clf.predict([input_vector])[0]\n","\n","# Show result\n","print(\"\\nYour sentence:\")\n","print(f\"  \\\"{input_sentence}\\\"\")\n","print(\"Predicted label:\", \"Legal\" if predicted_label == 1 else \"Poetic\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"snjoTUTELIeR","executionInfo":{"status":"ok","timestamp":1749665302623,"user_tz":300,"elapsed":9,"user":{"displayName":"zweige1003","userId":"02327896534499890410"}},"outputId":"b33fcbe4-0b3a-4b21-98f2-4c5458f49ed3"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Your sentence:\n","  \"Shall I compare thee to a summer's day?\"\n","Predicted label: Poetic\n"]}]},{"cell_type":"markdown","source":["## Logistic regression\n","Logistic Regression is a widely used classification model. It works by estimating the probability that a sentence belongs to a certain category (like legal or poetic), based on the words it contains. The model adjusts how important each word is during training and makes predictions by combining those learned weights."],"metadata":{"id":"so856pMLQXeE"}},{"cell_type":"code","source":["# Train Logistic Regression model\n","log_clf = LogisticRegression(max_iter=1000, random_state=42)\n","log_clf.fit(X_train, y_train)  # Labels stay as 0 (poetic) and 1 (legal)\n","\n","# Predict on the test set\n","log_y_pred = log_clf.predict(X_test)\n","log_accuracy = accuracy_score(y_test, log_y_pred)\n","print(f\"Logistic Regression Test Accuracy: {log_accuracy:.2f}\")"],"metadata":{"id":"77CGWT4FQXtG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749665749461,"user_tz":300,"elapsed":83,"user":{"displayName":"zweige1003","userId":"02327896534499890410"}},"outputId":"5f6c1847-9d5c-4e92-b33e-b91a263990b8"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression Test Accuracy: 0.97\n"]}]},{"cell_type":"code","source":["# Try your own sentence!\n","# Change the sentence below to test what the model predicts\n","input_sentence = \"To make Rules for the Government and Regulation of the land and naval Forces;\"\n","\n","# Convert to Bag-of-Words vector\n","input_vector = sentence_to_vector(input_sentence)\n","\n","# Predict probability\n","probs = log_clf.predict_proba([input_vector])[0]  # returns [prob_poetic, prob_legal]\n","\n","# Predict label\n","predicted_label = log_clf.predict([input_vector])[0]\n","\n","# Show result\n","print(\"\\nYour sentence:\")\n","print(f\"  \\\"{input_sentence}\\\"\")\n","print(f\"Predicted probabilities: Poetic = {probs[0]:.2f}, Legal = {probs[1]:.2f}\")\n","print(\"Predicted label:\", \"Legal\" if predicted_label == 1 else \"Poetic\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xv8YJCJeL95l","executionInfo":{"status":"ok","timestamp":1749666611189,"user_tz":300,"elapsed":11,"user":{"displayName":"zweige1003","userId":"02327896534499890410"}},"outputId":"ffbca248-c1aa-4fe8-8a7c-7802421d2c4c"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Your sentence:\n","  \"To make Rules for the Government and Regulation of the land and naval Forces;\"\n","Predicted probabilities: Poetic = 0.14, Legal = 0.86\n","Predicted label: Legal\n"]}]},{"cell_type":"markdown","source":["#### How do logistic regression model \"decide\"?\n","\n","Behind the scenes, Logistic Regression gives each word a weight based on how strongly it signals one category. In our case, positive weights push the sentence toward legal, and negative weights push toward poetic.\n","\n","The model then adds up the weights for the words it sees and turns that into a probability for each category."],"metadata":{"id":"CnSsIlWpVcg2"}},{"cell_type":"code","source":["#@title Top words in your sentence that influenced the prediction {display-mode: \"form\"}\n","# Show prediction result\n","print(\"\\nYour sentence:\")\n","print(f\"  \\\"{input_sentence}\\\"\")\n","print(f\"Predicted probabilities: Poetic = {probs[0]:.2f}, Legal = {probs[1]:.2f}\")\n","print(\"Predicted label:\", \"Legal\" if predicted_label == 1 else \"Poetic\")\n","\n","# Plot the probabilities\n","plt.bar([\"Poetic\", \"Legal\"], probs, color=[\"skyblue\", \"salmon\"])\n","plt.title(\"Logistic Regression Prediction Confidence\")\n","plt.ylabel(\"Probability\")\n","plt.ylim(0, 1)\n","plt.show()\n","\n","# Highlight influential words (non-zero words in input)\n","word_weights = log_clf.coef_[0]  # weights for each word (positive = legal, negative = poetic)\n","\n","print(\"\\nTop words in your sentence that influenced the prediction:\")\n","input_words = tokenize(input_sentence)\n","seen = set()\n","for word in input_words:\n","    if word in word_to_index and word not in seen:\n","        seen.add(word)\n","        idx = word_to_index[word]\n","        weight = word_weights[idx]\n","        direction = \"Legal\" if weight > 0 else \"Poetic\"\n","        print(f\"  {word:15} → weight = {weight:+.4f} → favors {direction}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":819},"id":"qfD6sVq7NPPb","executionInfo":{"status":"ok","timestamp":1749668052492,"user_tz":300,"elapsed":532,"user":{"displayName":"zweige1003","userId":"02327896534499890410"}},"outputId":"31cd873d-542f-4f9d-a9bc-20a50e42dca7"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Your sentence:\n","  \"All legislative powers shall be vested in a Congress of the United States.\"\n","Predicted probabilities: Poetic = 0.51, Legal = 0.49\n","Predicted label: Poetic\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Top words in your sentence that influenced the prediction:\n","  all             → weight = -0.0274 → favors Poetic\n","  legislative     → weight = +0.0163 → favors Legal\n","  powers          → weight = +0.0177 → favors Legal\n","  shall           → weight = +0.9522 → favors Legal\n","  be              → weight = +0.6297 → favors Legal\n","  vested          → weight = +0.0177 → favors Legal\n","  in              → weight = -0.1303 → favors Poetic\n","  a               → weight = +0.5016 → favors Legal\n","  congress        → weight = +0.0454 → favors Legal\n","  of              → weight = +0.6194 → favors Legal\n","  the             → weight = +1.1381 → favors Legal\n","  united          → weight = +0.7301 → favors Legal\n","  states.         → weight = +0.0003 → favors Legal\n"]}]},{"cell_type":"code","source":["df_data['Label'].value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"id":"oREaM-G7Oq9s","executionInfo":{"status":"ok","timestamp":1749666173903,"user_tz":300,"elapsed":22,"user":{"displayName":"zweige1003","userId":"02327896534499890410"}},"outputId":"cdcff4ba-2307-478c-cc3c-755c0eb1448d"},"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Label\n","0    140\n","1     55\n","Name: count, dtype: int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>Label</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>140</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>55</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"]},"metadata":{},"execution_count":62}]},{"cell_type":"markdown","source":["### Naive Bayes\n","\n","The model tries to decide:\n","**“Is this sentence more likely to be legal or poetic?”**\n","\n","\n","\n","#### **The Bayesian Idea**\n","\n","    Bayesian probability is all about updating your belief based on new evidence.\n","\n","Imagine you're reading a mystery sentence like:\n","\n","    “Shall powers be vested in a Congress...”\n","\n","You might initially guess poetic, but then you see “Congress” and update your guess to legal. That’s what Bayes’ rule does:\n","\n","$New\\ belief = (Old\\ belief) × (Evidence\\ strength)$\n","\n","In math, that’s Bayes’ rule:\n","\n","$P(Legal∣Words)∝P(Legal)×P(Words∣Legal)$\n","\n","\n","#### **What Makes It “Naive”?**\n","\n","The model pretends that each word in the sentence contributes independently.\n","\n","In reality, words are related (\"Congress\" often follows \"United States\"), but Naive Bayes ignores that — which makes it naive.\n","\n","Even though it ignores word order and grammar, it still works well when the two text types (poetic vs. legal) use clearly different vocabulary.\n","\n","#### **How It Works**\n","\n","1. **Training phase:**\n","\n","It counts how often each word appears in poetic and legal sentences.\n","\n","It also calculates how common each label is (e.g., 50% poetic, 50% legal).\n","\n","2. **Prediction phase:**\n","\n","For a new sentence, it checks which words appear.\n","\n","It multiplies the likelihoods:\n","\n","How common is \"shall\" in legal text?\n","\n","How common is \"thy\" in poetic text?\n","\n","It then picks the label with the highest score.\n","\n","3. **Example**\n","\n","A sentence contains: [\"shall\", \"thy\", \"beauty\"]\n","\n","Naive Bayes might find:\n","\n","\"shall\" appears 30 times in legal, 2 times in poetic\n","\n","\"thy\" appears 1 time in legal, 25 times in poetic\n","\n","\"beauty\" appears 0 times in legal, 15 times in poetic\n","\n","So:\n","\n","$Poetic\\ score = prior\\_poetic × (2×25×15)$\n","\n","$Legal\\ score = prior\\_legal × (30×1×0)$\n","\n","Since the word “beauty” never appeared in legal texts, that label’s score collapses to zero — and the model confidently chooses poetic."],"metadata":{"id":"F9CPhtvJCyoa"}},{"cell_type":"code","source":["# Train the Naive Bayes model\n","nb_clf = MultinomialNB()\n","nb_clf.fit(X_train, y_train)  # Labels stay as 0 = poetic, 1 = legal\n","\n","# Predict on the test set\n","nb_y_pred = nb_clf.predict(X_test)\n","nb_accuracy = accuracy_score(y_test, nb_y_pred)\n","print(f\"Naive Bayes Test Accuracy: {nb_accuracy:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HHKaZfsuMmyf","executionInfo":{"status":"ok","timestamp":1749666558964,"user_tz":300,"elapsed":19,"user":{"displayName":"zweige1003","userId":"02327896534499890410"}},"outputId":"2dfe5d29-3d69-49e9-c26b-16310e3175ab"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["Naive Bayes Test Accuracy: 0.92\n"]}]},{"cell_type":"code","source":["# Try your own sentence!\n","# Change the sentence below to test what the model predicts\n","input_sentence = \"Thy beauty shall no more be found in the grave.\"\n","\n","# Convert to Bag-of-Words vector\n","input_vector = sentence_to_vector(input_sentence)\n","\n","# Predict probability\n","probs = nb_clf.predict_proba([input_vector])[0]  # [prob_poetic, prob_legal]\n","predicted_label = nb_clf.predict([input_vector])[0]\n","\n","print(\"\\nYour sentence:\")\n","print(f\"  \\\"{input_sentence}\\\"\")\n","print(f\"Predicted probabilities: Poetic = {probs[0]:.2f}, Legal = {probs[1]:.2f}\")\n","print(\"Predicted label:\", \"Legal\" if predicted_label == 1 else \"Poetic\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_BVYxq-AQNKW","executionInfo":{"status":"ok","timestamp":1749666579334,"user_tz":300,"elapsed":7,"user":{"displayName":"zweige1003","userId":"02327896534499890410"}},"outputId":"1b630be7-76fb-4ab0-e544-94a22f12f688"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Your sentence:\n","  \"Thy beauty shall no more be found in the grave.\"\n","Predicted probabilities: Poetic = 0.51, Legal = 0.49\n","Predicted label: Poetic\n"]}]},{"cell_type":"markdown","source":["## Final Notes\n","\n","* We trained simple models to classify poetic vs legal sentences using only word counts (Bag-of-Words).\n","\n","* The Perceptron is a deterministic model: it always makes the same decision once trained.\n","\n","* Logistic Regression and Naive Bayes are probabilistic: they estimate the likelihood of each class and can handle ambiguity.\n","\n","* Model predictions depend on training data. If one class (e.g. poetic) dominates, the model may bias toward it.\n","\n","* These models don’t understand meaning — they rely only on word frequency and patterns.\n","\n","* These ideas form the foundation of how more advanced language models work today."],"metadata":{"id":"Q0drWIr4S0e_"}}]}